:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Workflows

toc::[]

.About CSV parsing errors
[NOTE]
====
`csvlint` is not Ruby 3-ready yet, so is not incorporated. I don't have time now to investigate integration of other potential CSV validation stuff. **If you get errors about your CSV not being readable or parseable, you'll need to look into it yourself.**
====

== Workflow overview
NOTE: strike-through formatting indicates the option/command is not yet implemented

* Produce CSV of record data using format of appropriate https://github.com/collectionspace/cspace-config-untangler/tree/main/data/templates[CSV template]
* Add batch to migration tool
** `thor batch:add --id obj12 --csv ~/path/to/source_data.csv --rectype collectionobject --action create`; OR
** Manually add a new row to the batches CSV, populating `id`, `source_csv`, `mappable_rectype`, and `action` columns. Then run `thor batches:fix_csv` to have the ``rec_ct` column populated by the tool.
* Map batch to CS XML
** `thor batch:map --id obj12`
* Upload resulting CS XML to S3 bucket
** +++<s>+++`thor batch:upload --id obj12`+++</s>+++
* Verify ingest is complete (see step details on this for more info/options)
* Run ingest verification report
** +++<s>+++`thor batch:verify --id obj12`+++</s>+++
* If any records failed to ingest, run +++<s>+++`batch:cb`+++</s>+++ (cb = clear bucket)
* If there were any duplicates added, run +++<s>+++`batch:dd`+++</s>+++ (dd = delete duplicates. See important notes on this in step details below)

=== Managing batches

The batches CSV isn't really intended as a permanent record of all work done in a given instance. It can be useful to keep some completed batches with errors or warnings in the batches CSV as a reminder to check/handle the records that didn't work as expected the first time. But, once you are done with a batch and tasks related to it, it is best to move it out of the batches CSV.

There are two options for moving completed batches out of the batches CSV:

delete:: Deletes batch directory and deletes row for batch from batches CSV
+++<s>+++archive+++</s>+++:: Zips batch directory and moves it to `batch_data/archives` directory. Moves row for batch from batches CSV to `batch_data/archives/archived_batches.csv`

The `delete` and `archive` commands can both be run:

* for a single batch (`thor batch:delete --id obj12`); OR
* on all completed batches (`thor batches:archive`)

IMPORTANT: A batch's source CSV may live anywhere on the user's machine. It is not copied into the batch directory, so archiving a batch does archive the source CSV itself. However, the mapper, upload, and ingest reports generated by this tool append columns onto the end of the original data, so you can re-constitute the original source data from any of those.

== Autocaching
IMPORTANT: This behavior is configurable and only applies when mapping is done as part of batch workflow.

There are two options related to this behavior:

`auto_refresh_cache_before_mapping` (in client_config.yml) or `--autocache` (per `thor batch:map` command):: **If true,** when you run `thor batch:map`, the first thing that will happen is fresh data for record types required by the type and data you are mapping will be pulled from the database and cached. **If false**, caches will NOT be cleared and refreshed prior to data mapping when called when `thor batch:map` is called. This may be preferable if the scale of the instance is very large, so it takes a long time to refresh cache data. If commented out in client_config.yml, defaults to `false`. Defaults to the value from client_config.yml if not explictly specified per batch via `--autocache`.

`clear_cache_before_refresh` (in client_config.yml) or `--clearcache` (per `thor batch:map` command):: **If true**, caches will be cleared before they are auto refreshed. This is the safest option, as it will ensure deleted items are not still in the cache, and the correct values are cached for any duplicates since cleaned up. If you know only additions/updates have been made, you can set to false to save a bit of time. This has no effect if `auto_refresh_cache_before_mapping` = `false`.

=== How autocaching works
If autocaching is enabled, the first step of the mapping process determining the refname and csid cache dependencies for the batch, querying the database for that data, and re-caching it. 

==== For objects, procedures, and authorities
The JSON record mapper for each record type specifies vocabulary or authority term sources and transformations for each possible field.

These fields will need to be populated with the refnames of existing terms, so the refname cache needs to be populated.

To get the record status of (new or existing) of each record being mapped, we need the csids for the record type of the batch. 

The only csid cache dependency will be the record type of the batch.

The headers of the batch's source CSV are compared against the column mappings in the JSON record mapper. The names of any vocabularies or authorities used to control values in any columns included in your CSV are pulled out and deduplicated to populate the `refname_dependencies` column for the batch.

==== For relations
Though it is possible to create relations by specifying subject and object refnames, relations specifying subject and object csids appear to ingest a bit faster.

This tool creates all relations by looking up the cached csids for the object id, procedure id, or first occurring authority termDisplayName values given.

Thus, there will be no refname cache dependencies for relations.

===== authorityhierarchy

Each row in the source CSV requires you specify the `termType` and `termSubType` for the relationship.

TIP: This means you can import hierarchies for multiple authority vocabularies in one batch. However, the broader/narrower terms on each row must always be in the same authority vocabulary.

The tool grabs the unique combined `termType`/`termSubType` values that appear in the batch source CSV, and adds these to `authorityhierarchy` as csid cache dependencies.

===== nonhierarchicalrelationship
These currently can only be object/object, object/procedure, or procedure/procedure.

Each row of the batch source CSV requires `item1_type` and `item2_type` values.

The tool grabs all unique values from these columns in the batch source CSV and adds them to `nonhierarchicalrelationship` as csid cache dependencies.

===== objecthierarchy

objecthierarchy and collectionobject


== Step details
=== Map batch to CS XML
As part of batch workflow: `thor batch:map --id obj12`

For testing/standalone work: `thor map:csv --csv ~/path/to/source_data.csv --rectype collectionobject --action create`

==== Creates batch directory
If part of batch workflow, batch directory name is the batch id.

Otherwise batch directory is named with timestamp+rectype.

==== Preprocesses data
Verifies the following:

* The given CSV exists
* The first row can be parsed from the given CSV
* There are no headerless fields
* Header for required field is present

If any of the above are not true, the process will stop and you are informed of the problem.

Also:

* checks all fields/headers and
** prints count and list of known fields that can be mapped
** prints a warning listing any unknown fields that will be ignored (this may indicate a misnamed column)

==== Processes data
This step is batched (using SmarterCSV to read your CSV data in chunks) and multi-process. Each chunk is passed to a forked process, which sequentially processes each row in the chunk.

We use multiple processes rather than threads because this work is more CPU-bound than IO-bound.

Each row is passed to `collectionspace-mapper` which returns a `CollectionSpace::Mapper::Response` object that wraps the resulting XML (if it could be created), as well as any errors or warnings raised in the mapping process, and information on the record status in the given CS instance. If it is an existing record, the `Response` includes the record csid and uri for use in any subsequent API calls on the record.

==== Writes successfully mapped records to individual XML files

Successfully mapped records are written into the batch directory. The file name is the record identifier, Base64 encoded. This encoding is necessary because:

* some characters frequently used in record id values are not file name safe; and
* we leverage checking whether a file already exists before writing to avoid (and flag) records with duplicate identifiers in a batch.

If there are multiple records with the same ID in your batch, the first record will be fine. Subsequent records with the same ID will be treated as errors.

==== Reports
===== `mapper_report.csv`
Includes the following columns:

* all columns from source csv
* `cmt_rec_status`: record status from `CollectionSpace::Mapper::Response` (new or existing)
* `cmt_outcome`: `success` if XML was created and saved; `failure` if not
* `cmt_output_file`: name of XML file if created (so you can find a specific record easily)
* `cmt_s3_key`: string that will be used as the AWS S3 object key when XML file is uploaded to bucket. This is a Base64 url-safe encoded string created from:
** +++<s>+++batch id+++</s>+++
** services api path (includes record csid for updates/deletes, includes blobUri for media with files to ingest)
** identifier
** action (will control what API method is used to transfer record)
* `cmt_warnings`: issues to be aware of. They may be fine, or they may indicate something unexpected is going on. Note that you will see a warning here if:
** the batch has action = create, but the record status is existing
** the batch has action = update, but the record status is new
* `cmt_errors`: why a record mapping failed

IMPORTANT: You can continue to the next step if individual records fail. Those records will just be skipped in subsequent steps

===== `missing_terms.csv`

==== `batches.csv`
Populates the following columns:

* `mapped?` - timestamp entered
* `dir` - batch directory
* `map_errs` - the number of records with mapping errors
* `map_oks` - the number of records successfully mapped
* `map_warns` - the number of successfully mapped records with warnings

=== Upload
==== Media Handling

You can transfer media and import files by including a URI in `mediaFileURI` column of your CSV. This works for:

* new media records created
* existing media records updated -- If existing media records have blobs attached they will be unattached and replaced by the new blob given.
