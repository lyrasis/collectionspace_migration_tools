:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Workflows

toc::[]

.About CSV parsing errors
[NOTE]
====
`csvlint` is not Ruby 3-ready yet, so is not incorporated. I don't have time now to investigate integration of other potential CSV validation stuff. **If you get errors about your CSV not being readable or parseable, you'll need to look into it yourself.**
====

== Workflow overview
NOTE: strike-through formatting indicates the option/command is not yet implemented

* Produce CSV of record data using format of appropriate https://github.com/collectionspace/cspace-config-untangler/tree/main/data/templates[CSV template]
* Add batch to migration tool
** `thor batch:add --id obj12 --csv ~/path/to/source_data.csv --rectype collectionobject --action create`; OR
** Manually add a new row to the batches CSV, populating `id`, `source_csv`, `mappable_rectype`, and `action` columns. Then run `thor batches:fix_csv` to have the `rec_ct` column populated by the tool.
* Map batch to CS XML
** `thor batch:map obj12`
** Examine warnings (`thor batch:map_warnings obj12`, thor batch:map_warnings obj12 --with_counts`, or in `mapper_report.csv`)
* Upload resulting CS XML to Fast Importer Bucket
** `thor batch:upload obj12`
* Run ingest status report - may need to be run repeatedly, if a batch takes a long time to be processed
** `thor batch:ingstat obj12`
* If any records failed to ingest, run `batch:cb BATCHID` (cb = clear bucket) after doing any troubleshooting/etc
* If there were any duplicates added, run `duplicates:delete RECTYPE` (See important notes on this in step details below)
* Once `batch:ingstat` has indicated the ingest is complete, the batch status is set to "ingested", which indicates the batch is done.
* Delete completed batches (`batch:delete BATCHID` or `batches:delete_done`)

[TIP]
====
You can't run a step on a batch if you have already run that step.

For example, if you get some warnings/errors when mapping batch 1, resolve the issues, and want to map again, you will get an error.

Each step thus has a rollback (rb) command:

* `thor batch:rb_map BATCHID`
* `thor batch:rb_upload BATCHID`
* `thor batch:rb_ingstat BATCHID`

These clear any data entered in the batches.csv, and delete any step-specific files.
====

[TIP]
====
I've found that if an ingest initially appears to be complete with errors, if I let it sit there for longer, sometimes the errors have been resolved. I think this is related to delay on re-trying errors in the S3 ingest process. The process for rechecking ingest status would be:

`thor batch:rb_ingstat BATCHID`
`thor batch:ingstat BATCHID`
====

=== Managing batches
==== Listing batches in given workflow state

* `thor batches:to_map`
* `thor batches:to_upload`
* `thor batches:to_ingstat`
* `thor batches:done`

==== Running `map`, `upload`, or `ingstat` on all batches ready

* `thor batches:map`
* `thor batches:upload`
* `thor batches:ingstat`

==== Cleaning out finished batches
The batches CSV isn't intended as a permanent record of all work done in a given instance. It can be useful to keep some completed batches with errors or warnings in the batches CSV as a reminder to check/handle the records that didn't work as expected the first time. But, once you are done with a batch and tasks related to it, it is best to delete the batch.


There are two options for moving completed batches out of the batches CSV:

`thor batch:delete BATCHID`:: Delete the specified batch. *This command deletes the batch regardless of batch status*
`thor batches:delete_done`:: Delete all batches with `ingested` batch status

The following happens when a batch is deleted:

* The batch directory containing XML records and reports is deleted.
* The batches CSV row for the batch will be deleted
* If the batch status was `ingested`, and you are archiving batches, the batches CSV row for the batch will be written to the batch archive CSV

IMPORTANT: A batch's source CSV may live anywhere on the user's machine. It is not copied into the batch directory, so deleting the batch does not delete the original file from which the ingested XML was derived. The mapper, upload, and ingest reports generated by this tool are written to the batch directory, and will be deleted with the batch

==== Batch archiving

Unless you add `archive_batches: false` in your client config, batch archiving is enabled.

Batch archiving means: when an *ingested* batch is deleted, the row for that batch is removed from the batches CSV and written to the batch archive CSV. Deleted batches with `added`, `mapped`, or `uploaded` status are not archived.

**Now that running `batch:ingstat BATCHID` calculates ingest duration, retaining all ingested batches in the batch archive CSV allows us to capture data on ingest performance with no effort.** It behooves us to retain this information, because "well, how did it used to perform?" is often the first question if we raise a performance complaint.

The batches archive CSV is always created in your client base directory. The default file name is `batches_archive.csv`, but you can change this in the `batch_archive_filename` setting in the client config.

== Autocaching
IMPORTANT: This behavior is configurable and only applies when mapping is done as part of batch workflow.

There are two options related to this behavior:

`auto_refresh_cache_before_mapping` (in client_config.yml) or `--autocache` (per `thor batch:map` command):: **If true,** when you run `thor batch:map`, the first thing that will happen is fresh data for record types required by the type and data you are mapping will be pulled from the database and cached. **If false**, caches will NOT be cleared and refreshed prior to data mapping when called when `thor batch:map` is called. This may be preferable if the scale of the instance is very large, so it takes a long time to refresh cache data. If commented out in client_config.yml, defaults to `false`. Defaults to the value from client_config.yml if not explictly specified per batch via `--autocache`.

`clear_cache_before_refresh` (in client_config.yml) or `--clearcache` (per `thor batch:map` command):: **If true**, caches will be cleared before they are auto refreshed. This is the safest option, as it will ensure deleted items are not still in the cache, and the correct values are cached for any duplicates since cleaned up. If you know only additions/updates have been made, you can set to false to save a bit of time. This has no effect if `auto_refresh_cache_before_mapping` = `false`.

=== How autocaching works
If autocaching is enabled, the first step of the mapping process involves:

* determining the refname and csid cache dependencies for the batch, based on record type and fields populated
* querying the database for that data
* re-caching the query results

==== For objects, procedures, and authorities
The JSON record mapper for each record type specifies vocabulary or authority term sources and transformations for each possible field.

These fields will need to be populated with the refnames of existing terms, so the refname cache needs to be populated.

To get the record status of (new or existing) of each record being mapped, we need the csids for the record type of the batch.

The only csid cache dependency will be the record type of the batch.

The headers of the batch's source CSV are compared against the column mappings in the JSON record mapper. The names of any vocabularies or authorities used to control values in any columns included in your CSV are pulled out and deduplicated to populate the `refname_dependencies` column for the batch.

==== For relations
Though it is possible to create relations by specifying subject and object refnames, relations specifying subject and object csids appear to ingest a bit faster.

This tool creates all relations by looking up the cached csids for the object id, procedure id, or first occurring authority termDisplayName values given.

Thus, there will be no refname cache dependencies for relations.

===== authorityhierarchy

Each row in the source CSV requires you specify the `termType` and `termSubType` for the relationship.

TIP: This means you can import hierarchies for multiple authority vocabularies in one batch. However, the broader/narrower terms on each row must always be in the same authority vocabulary.

The tool grabs the unique combined `termType`/`termSubType` values that appear in the batch source CSV, and adds these to `authorityhierarchy` as csid cache dependencies.

===== nonhierarchicalrelationship
These currently can only be object/object, object/procedure, or procedure/procedure.

Each row of the batch source CSV requires `item1_type` and `item2_type` values.

The tool grabs all unique values from these columns in the batch source CSV and adds them to `nonhierarchicalrelationship` as csid cache dependencies.

===== objecthierarchy

objecthierarchy and collectionobject CSIDs are cached


== Step details
=== Map batch to CS XML
As part of batch workflow: `thor batch:map --id obj12`

For testing/standalone work: `thor map:csv --csv ~/path/to/source_data.csv --rectype collectionobject --action create`

==== Creates batch directory
If part of batch workflow, batch directory name is the batch id.

Otherwise batch directory is named with timestamp+rectype.

==== Autocaches (if configured to do so)
See <<autocaching>>

==== Preprocesses data
Verifies the following:

* The given CSV exists
* The first row can be parsed from the given CSV
* There are no headerless fields
* Header for required field is present

If any of the above are not true, the process will stop and you are informed of the problem.

Also:

* checks all fields/headers and
** prints count and list of known fields that can be mapped
** prints a warning listing any unknown fields that will be ignored (this may indicate a misnamed column)

==== Processes data
This step is batched (using SmarterCSV to read your CSV data in chunks) and multi-process. Each chunk is passed to a forked process, which sequentially processes each row in the chunk.

We use multiple processes rather than threads because this work is more CPU-bound than IO-bound.

Each row is passed to `collectionspace-mapper` which returns a `CollectionSpace::Mapper::Response` object that wraps the resulting XML (if it could be created), as well as any errors or warnings raised in the mapping process, and information on the record status in the given CS instance. If it is an existing record, the `Response` includes the record csid and uri for use in any subsequent API calls on the record.

==== Writes successfully mapped records to individual XML files

Successfully mapped records are written into the batch directory. The file name is the record identifier, Base64 encoded. This encoding is necessary because:

* some characters frequently used in record id values are not file name safe; and
* we leverage checking whether a file already exists before writing to avoid (and flag) records with duplicate identifiers in a batch.

If there are multiple records with the same ID in your batch, the first record will be fine. Subsequent records with the same ID will be treated as errors.

==== Reports
===== `mapper_report.csv`
Includes the following columns:

* all columns from source csv
* `cmt_rec_status`: record status from `CollectionSpace::Mapper::Response` (new or existing)
* `cmt_outcome`: `success` if XML was created and saved; `failure` if not
* `cmt_output_file`: name of XML file if created (so you can find a specific record easily for review)
* `cmt_s3_key`: string that will be used as the AWS S3 object key for the XML file when it is uploaded to the Fast Importer Bucket. This is a Base64 url-safe encoded string created from:
** batch id
** services api path (includes record csid for updates/deletes; blobUri for media with files to ingest)
** identifier
** action (will control what API method is used to transfer record)
* `cmt_warnings`: issues to be aware of. They may be fine, or they may indicate something unexpected is going on. Note that you will see a warning here if:
** the batch has action = create, but the record status is existing
** the batch has action = update, but the record status is new
* `cmt_errors`: why a record mapping failed

IMPORTANT: You can continue to the next step if individual records fail. Those records will just be skipped in subsequent steps

===== `missing_terms.csv`

If any records failed to map because refnames were not found for authority or vocbulary terms, the unique individual terms not found are reported here.

This CSV can be sorted by expected term source vocabulary, and the contents used to create source CSVs for necessary authority term ingests.

==== `batches.csv`
Populates the following columns:

* `mapped?` - timestamp entered
* `dir` - batch directory
* `map_errs` - the number of records with mapping errors
* `map_oks` - the number of records successfully mapped
* `map_warns` - the number of successfully mapped records with warnings

Also prints this info to the screen at the end of the batch run.

==== Map step followups
===== ESSENTIAL: Review warnings if `map_warns` is non-zero

*This is the most important thing to do before uploading*

Records with warnings _will_ be uploaded/ingested, so the warnings might indicate something that could actually be a problem.

The most important warning category is batch action/record status mismatch. For example, if the batch action is `create`, but the record is found to exist, the action for that record is set to `update` but you are warned about it.

This may be convenient and expected, OR it may be unexpected and problematic if such records are indeed ingested as updates.

Command to check what warnings there are from command line:

`thor batch map_warnings batchname`

or

`thor batch map_warnings --with-counts batchname`

For details on which records got what warnings, consult mapper_report.csv for the batch.

===== Review errors if `map_errs` is non-zero

These records will not be uploaded, so the thing here is to decide if you want to fix whatever issues caused errors now and restart this as a single batch, or whether you want to move ahead with uploading the successes from this batch, and handling errors as a separate set.

=== Upload successfully mapped records to Fast Importer Bucket

The upload of a record to the bucket triggers an AWS Lambda function to attempt ingest of the record via the API.
The information Lambda function decodes the object key of each item uploaded to derive the exact API call to make for that item.

As part of batch workflow: `thor batch:upload obj12`

For testing/standalone work: `thor upload:dir DIRNAME`

If running the testing/standalone command, DIRNAME should be the name of a directory in whatever you have as `batch_dir` in your client config. This directory should contain .xml files and a `mapper_report.csv`.

This is more of an IO-bound, rather than CPU-bound process, so it runs in threads.

It reads in `mapper_report.csv`, ignores any rows where mapping failed, and writes the contents of each `cmt_output_file` to the S3 bucket with the object key in `cmt_s3_key`.

.About the object key
[NOTE]
====
This key is produced by concatenating (with `s3_delimiter` value as separator):

* batch id
* service path for API call
* human readable id for record
* action (create, update, or delete)

The concatenated string is then URL-safe Base64 encoded with padding.
====

==== Reports
===== `upload_report.csv`
Includes the following columns:

* all columns from `mapper_report.csv`
* `cmt_upload_status`: values include:
** `skip`: the record had mapping errors
** `unuploadable`: the record was missing info required to upload it
** `failure`: upload was initiated but the expected response was not received
** `success`: upload was initiated and the expected response was received
* `cmt_upload_message`: may give more info about failure or un-uploadable-ness

==== `batches.csv`
Populates the following columns:

* `uploaded?` - timestamp of upload process
* `upload_oks` - the number of records successfully uploaded
* `upload_errs` - the number of records with mapping errors, or that were skipped for being unuploadable
* `batch_prefix` - The initial part of the object key representing the batch id. This is later used to check whether there are still any objects from this batch in the S3 bucket

Also prints this info to the screen at the end of the batch run.

==== Media Handling

Media Handling procedures without blobs are ingested normally.

Media Handling procedures with blobs can be ingested, but there are some special config settings and commands to support flakiness around batch ingest of files and derivative generation. These are documented on a https://github.com/lyrasis/collectionspace_migration_tools/blob/main/doc/media.adoc[separate page].


==== Upload step followups
I have not run into enough problems on this step while developing/testing to see anything that can be systematized.

Clearly you'll need to check out any reported upload errors.

The number of reported ok uploads plus the number of reported upload errors should equal the number of successfully mapped records. There is https://github.com/lyrasis/collectionspace_migration_tools/issues/2[an issue] to add verification of this to the post-upload reporting.

=== Verify ingest completion and status

NOTE: There is no standalone/test command for this, since the entire functionality of this step depends on the context of a batch.

In this step we do our best to determine:

- that all objects in a batch have been processed
- whether there were any failures and why
- whether any duplicates were added due to race conditions in the AWS Lambda processing

This step is a little tricky because the S3/Lambda side of things has no concept of a batch at all. We fake that by prefixing the object keys with the batch id.

Also, the AWS side of things generates logs but does not report back anything coherent about failures. At last check, the log message containing the actual reason for an ingest failure did not also contain the S3 object key or anything else we can easily use to connect an error message to information on our end.

Expectations/assumptions:

- Successfully ingested records are removed from the S3 bucket
- Unsuccessfully ingested records remain in the S3 bucket

Based on this, we further assume:

. If the number of objects from the batch still in S3 bucket is changing, ingest is incomplete
. If the number of objects from the batch still in S3 bucket is zero, ingest is complete and all records were ingested successfully
. If the number of objects from the batch still in S3 bucket is no longer changing, but is non-zero, ingest is complete and the remaining records were not ingested successfully

The options for calling `thor batch:ingstat` are rather complex, and are fully described if you do `thor help batch:ingstat` or just read https://github.com/lyrasis/collectionspace_migration_tools/blob/main/lib/tasks/batch_ingstat.txt[the source for that help text]. Additionally, https://github.com/lyrasis/collectionspace_migration_tools/blob/main/spec/collectionspace_migration_tools/batch/ingest_status_checker_spec.rb[the automated tests for the ingest status checker] were written with the intent of reminding me what the heck it does when I have been away from this code for a while.

If you call `thor batch:ingstat` and the result is #1 from the above list, it tells you the ingest is not complete, and how many objects were in the bucket the last time it checked.

Otherwise, it determines the ingest is complete and writes the ingest related column values to `batches.csv` If the situation is #3 from the above list, it generates `ingest_report.csv`.

Then, unless this batch is for relation records, we run a database query to check for duplicate records of the batch's record type. If any duplicates are found, they are written to `duplicate_report.csv`.

IMPORTANT: Please refer to https://github.com/lyrasis/collectionspace_migration_tools/blob/main/doc/duplicates.adoc[the duplicates background doc] for the important assumptions and caveats here.

If you ran `thor batch:ingstat` with the `--dupedelete` flag, delete batches will be automatically created and run until there are no more duplicates.

==== Reports
===== `ingest_report.csv`
*Only generated if there are objects for the batch remaining in the S3 bucket and the number of those objects is no longer changing*

Includes the following columns:

* all columns from `upload_report.csv`
* `cmt_ingest_status`: failure (if object is still in S3 bucket), or success

===== `duplicate_report.csv`
*Only generated if duplicate records for the ingested record type are found in the database after ingest*

Contains one column. The header is the field name of the human-readable identifier field for the record type. The values in that column are any identifier values used in more than one record.

===== `batches.csv`
Populates the following columns:

* `ingest_done?` - timestamp of ingest check that determined ingest was finished
* `ingest_oks` - the number of records assumed to be successfully ingested
* `ingest_errs` - the number of records whose objects remain in the S3 bucket, indicating they were presumably not successfully ingested


Also prints this info to the screen at the end of the batch run.

=== Media Handling

You can transfer media and import files by including a URI in `mediaFileURI` column of your CSV. This works for:

* new media records created
* existing media records updated -- If existing media records have blobs attached they will be unattached and replaced by the new blob given.

WARNING: Ingesting records that trigger blob ingests remains flakier than ingesting other records. The speed at which records are ingested via this tool may be more likely to trigger throttling on the image downloading side. There are issues with ingesting blobs and finding/fixing failures even with CSV Importer, so there's still work to do...

=== Upload step followups / ingest errors
I have not run into enough problems on this step while developing/testing to see anything that can be systematized.

Clearly you'll need to check out any reported upload errors.

The number of reported ok uploads plus the number of reported upload errors should equal the number of successfully mapped records. There is https://github.com/lyrasis/collectionspace_migration_tools/issues/2[an issue] to add verification of this to the post-upload reporting.

==== `thor decode:objects` command

This decodes the S3 object keys of all objects in the S3 bucket and writes them to a CSV.

This can be a shortcut for finding out the human-readable ids (i.e. objectnumber or acquisitionreferencenumber) of records that failed to ingest.

==== Invalid data in a structured date group

The following in XML payload caused ingest error with 400 code:

[source, xml]
====
<workDateGroupList>
  <workDateGroup>
    <dateDisplayDate>1881-</dateDisplayDate>
    <scalarValuesComputed>false</scalarValuesComputed>
    <dateEarliestScalarValue>1881-</dateEarliestScalarValue>
  </workDateGroup>
</workDateGroupList>
====

I believe this is caused by an invalid `dateEarliestScalarValue` value. `collectionspace-mapper` shouldn't be setting that value that way, so I am making an issue there.
