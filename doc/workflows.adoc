:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Workflows

toc::[]

.About CSV parsing errors
[NOTE]
====
`csvlint` is not Ruby 3-ready yet, so is not incorporated. I don't have time now to investigate integration of other potential CSV validation stuff. **If you get errors about your CSV not being readable or parseable, you'll need to look into it yourself.**
====

== Workflow overview
NOTE: strike-through formatting indicates the option/command is not yet implemented

* Produce CSV of record data using format of appropriate https://github.com/collectionspace/cspace-config-untangler/tree/main/data/templates[CSV template]
* Add batch to migration tool
** `thor batch:add --id obj12 --csv ~/path/to/source_data.csv --rectype collectionobject --action create`; OR
** Manually add a new row to the batches CSV, populating `id`, `source_csv`, `mappable_rectype`, and `action` columns. Then run `thor batches:fix_csv` to have the `rec_ct` column populated by the tool.
* Map batch to CS XML
** `thor batch:map obj12`
** Examine warnings (`thor batch:map_warnings obj12`, thor batch:map_warnings obj12 --with_counts`, or in `mapper_report.csv`)
* Upload resulting CS XML to S3 bucket
** `thor batch:upload obj12`
* Run ingest status report - may need to be run repeatedly, if a batch takes a long time to be processed
** `thor batch:ingstat obj12`
* If any records failed to ingest, run `batch:cb BATCHID` (cb = clear bucket) after doing any troubleshooting/etc
* If there were any duplicates added, run `duplicates:delete RECTYPE` (See important notes on this in step details below)
* Mark completed batches as done (`batches:mark_done`)
* Delete completed batches (`batches:delete_done`)

[TIP]
====
You can't run a step on a batch if you have already run that step.

For example, if you get some warnings/errors when mapping batch 1, resolve the issues, and want to map again, you will get an error.

Each step thus has a rollback (rb) command:

* `thor batch:rb_map BATCHID`
* `thor batch:rb_upload BATCHID`
* `thor batch:rb_ingstat BATCHID`

These clear any data entered in the batches.csv, and delete any step-specific files.
====

[TIP]
====
I've found that if an ingest initially appears to be complete with errors, if I let it sit there for longer, sometimes the errors have been resolved. I think this is related to delay on re-trying errors in the S3 ingest process. The process for rechecking ingest status would be:

`thor batch:rb_ingstat BATCHID`
`thor batch:ingstat BATCHID`
====

=== Managing batches
==== Listing batches in given workflow state

* `thor batches:to_map`
* `thor batches:to_upload`
* `thor batches:to_ingcheck`

==== Running `map`, `upload`, or `ingstat` on all batches ready

* `thor batches:map`
* `thor batches:upload`
* `thor batches:ingcheck`

==== Cleaning out finished batches
The batches CSV isn't really intended as a permanent record of all work done in a given instance. It can be useful to keep some completed batches with errors or warnings in the batches CSV as a reminder to check/handle the records that didn't work as expected the first time. But, once you are done with a batch and tasks related to it, it is best to move it out of the batches CSV.

There are two options for moving completed batches out of the batches CSV:

delete:: Deletes batch directory and deletes row for batch from batches CSV
+++<s>+++archive+++</s>+++:: Zips batch directory and moves it to `batch_data/archives` directory. Moves row for batch from batches CSV to `batch_data/archives/archived_batches.csv`

The `delete` and +++<s>+++`archive`+++</s>+++ commands can both be run:

* for a single batch (`thor batch:delete obj12`); OR
* on all completed batches (`thor batches:delete_done`). This one assumes you have run `thor batches:mark_done` and/or manually marked batches done in the batches CSV

IMPORTANT: A batch's source CSV may live anywhere on the user's machine. It is not copied into the batch directory, so archiving a batch does archive the source CSV itself. However, the mapper, upload, and ingest reports generated by this tool append columns onto the end of the original data, so you can re-constitute the original source data from any of those.

== Autocaching
IMPORTANT: This behavior is configurable and only applies when mapping is done as part of batch workflow.

There are two options related to this behavior:

`auto_refresh_cache_before_mapping` (in client_config.yml) or `--autocache` (per `thor batch:map` command):: **If true,** when you run `thor batch:map`, the first thing that will happen is fresh data for record types required by the type and data you are mapping will be pulled from the database and cached. **If false**, caches will NOT be cleared and refreshed prior to data mapping when called when `thor batch:map` is called. This may be preferable if the scale of the instance is very large, so it takes a long time to refresh cache data. If commented out in client_config.yml, defaults to `false`. Defaults to the value from client_config.yml if not explictly specified per batch via `--autocache`.

`clear_cache_before_refresh` (in client_config.yml) or `--clearcache` (per `thor batch:map` command):: **If true**, caches will be cleared before they are auto refreshed. This is the safest option, as it will ensure deleted items are not still in the cache, and the correct values are cached for any duplicates since cleaned up. If you know only additions/updates have been made, you can set to false to save a bit of time. This has no effect if `auto_refresh_cache_before_mapping` = `false`.

=== How autocaching works
If autocaching is enabled, the first step of the mapping process involves:

* determining the refname and csid cache dependencies for the batch, based on record type and fields populated
* querying the database for that data
* re-caching the query results 

==== For objects, procedures, and authorities
The JSON record mapper for each record type specifies vocabulary or authority term sources and transformations for each possible field.

These fields will need to be populated with the refnames of existing terms, so the refname cache needs to be populated.

To get the record status of (new or existing) of each record being mapped, we need the csids for the record type of the batch. 

The only csid cache dependency will be the record type of the batch.

The headers of the batch's source CSV are compared against the column mappings in the JSON record mapper. The names of any vocabularies or authorities used to control values in any columns included in your CSV are pulled out and deduplicated to populate the `refname_dependencies` column for the batch.

==== For relations
Though it is possible to create relations by specifying subject and object refnames, relations specifying subject and object csids appear to ingest a bit faster.

This tool creates all relations by looking up the cached csids for the object id, procedure id, or first occurring authority termDisplayName values given.

Thus, there will be no refname cache dependencies for relations.

===== authorityhierarchy

Each row in the source CSV requires you specify the `termType` and `termSubType` for the relationship.

TIP: This means you can import hierarchies for multiple authority vocabularies in one batch. However, the broader/narrower terms on each row must always be in the same authority vocabulary.

The tool grabs the unique combined `termType`/`termSubType` values that appear in the batch source CSV, and adds these to `authorityhierarchy` as csid cache dependencies.

===== nonhierarchicalrelationship
These currently can only be object/object, object/procedure, or procedure/procedure.

Each row of the batch source CSV requires `item1_type` and `item2_type` values.

The tool grabs all unique values from these columns in the batch source CSV and adds them to `nonhierarchicalrelationship` as csid cache dependencies.

===== objecthierarchy

objecthierarchy and collectionobject CSIDs are cached


== Step details
=== Map batch to CS XML
As part of batch workflow: `thor batch:map --id obj12`

For testing/standalone work: `thor map:csv --csv ~/path/to/source_data.csv --rectype collectionobject --action create`

==== Creates batch directory
If part of batch workflow, batch directory name is the batch id.

Otherwise batch directory is named with timestamp+rectype.

==== Autocaches (if configured to do so)
See <<autocaching>>

==== Preprocesses data
Verifies the following:

* The given CSV exists
* The first row can be parsed from the given CSV
* There are no headerless fields
* Header for required field is present

If any of the above are not true, the process will stop and you are informed of the problem.

Also:

* checks all fields/headers and
** prints count and list of known fields that can be mapped
** prints a warning listing any unknown fields that will be ignored (this may indicate a misnamed column)

==== Processes data
This step is batched (using SmarterCSV to read your CSV data in chunks) and multi-process. Each chunk is passed to a forked process, which sequentially processes each row in the chunk.

We use multiple processes rather than threads because this work is more CPU-bound than IO-bound.

Each row is passed to `collectionspace-mapper` which returns a `CollectionSpace::Mapper::Response` object that wraps the resulting XML (if it could be created), as well as any errors or warnings raised in the mapping process, and information on the record status in the given CS instance. If it is an existing record, the `Response` includes the record csid and uri for use in any subsequent API calls on the record.

==== Writes successfully mapped records to individual XML files

Successfully mapped records are written into the batch directory. The file name is the record identifier, Base64 encoded. This encoding is necessary because:

* some characters frequently used in record id values are not file name safe; and
* we leverage checking whether a file already exists before writing to avoid (and flag) records with duplicate identifiers in a batch.

If there are multiple records with the same ID in your batch, the first record will be fine. Subsequent records with the same ID will be treated as errors.

==== Reports
===== `mapper_report.csv`
Includes the following columns:

* all columns from source csv
* `cmt_rec_status`: record status from `CollectionSpace::Mapper::Response` (new or existing)
* `cmt_outcome`: `success` if XML was created and saved; `failure` if not
* `cmt_output_file`: name of XML file if created (so you can find a specific record easily for review)
* `cmt_s3_key`: string that will be used as the AWS S3 object key when XML file is uploaded to bucket. This is a Base64 url-safe encoded string created from:
** batch id
** services api path (includes record csid for updates/deletes, includes blobUri for media with files to ingest)
** identifier
** action (will control what API method is used to transfer record)
* `cmt_warnings`: issues to be aware of. They may be fine, or they may indicate something unexpected is going on. Note that you will see a warning here if:
** the batch has action = create, but the record status is existing
** the batch has action = update, but the record status is new
* `cmt_errors`: why a record mapping failed

IMPORTANT: You can continue to the next step if individual records fail. Those records will just be skipped in subsequent steps

===== `missing_terms.csv`

If any records failed to map because refnames were not found for authority or vocbulary terms, the unique individual terms not found are reported here.

This CSV can be sorted by expected term source vocabulary, and the contents used to create source CSVs for necessary authority term ingests.

NOTE: This list includes terms from vocabularies/dynamic term lists,footnote:[Distinct from terms from a specific authority vocabulary, such as person-ulan] but it is not yet possible to batch ingest vocabulary terms. 

==== `batches.csv`
Populates the following columns:

* `mapped?` - timestamp entered
* `dir` - batch directory
* `map_errs` - the number of records with mapping errors
* `map_oks` - the number of records successfully mapped
* `map_warns` - the number of successfully mapped records with warnings

Also prints this info to the screen at the end of the batch run.

==== Map step followups
===== ESSENTIAL: Review warnings if `map_warns` is non-zero

*This is the most important thing to do before uploading*

Records with warnings _will_ be uploaded/ingested, so the warnings might indicate something that could actually be a problem.

The most important warning category is batch action/record status mismatch. For example, if the batch action is `create`, but the record is found to exist, the action for that record is set to `update` but you are warned about it.

This may be convenient and expected, OR it may be unexpected and problematic if such records are indeed ingested as updates.

===== Review errors if `map_errs` is non-zero

These records will not be uploaded, so the thing here is to decide if you want to fix whatever issues caused errors now and restart this as a single batch, or whether to want to move ahead with uploading the successes from this batch, and handling errors as a separate set.

=== Upload successfully mapped records to S3 bucket
The upload of a record to the S3 bucket triggers an AWS Lambda function to attempt ingest of the record.

As part of batch workflow: `thor batch:upload obj12`

For testing/standalone work: `thor upload:dir DIRNAME`

If running the testing/standalone command, DIRNAME should be the name of a directory in whatever you have as `batch_dir` in your client config. This directory should contain .xml files and a `mapper_report.csv`.

This is more of an IO-bound, rather than CPU-bound process, so it runs in threads. 

It reads in `mapper_report.csv`, ignores any rows where mapping failed, and writes the contents of each `cmt_output_file` to the S3 bucket with the object key in `cmt_s3_key`.

.About the object key
[NOTE]
====
This key is produced by concatenating (with `s3_delimiter` value as separator):

* batch id
* service path for API call
* human readable id for record
* action (create, update, or delete)

The concatenated string is then URL-safe Base64 encoded with padding.
====

==== Reports
===== `upload_report.csv`
Includes the following columns:

* all columns from `mapper_report.csv`
* `cmt_upload_status`: values include:
** `skip`: the record had mapping errors
** `unuploadable`: the record was missing info required to upload it
** `failure`: upload was initiated but the expected response was not received
** `success`: upload was initiated and the expected response was received
* `cmt_upload_message`: may give more info about failure or un-uploadable-ness

==== `batches.csv`
Populates the following columns:

* `uploaded?` - timestamp of upload process
* `upload_oks` - the number of records successfully uploaded
* `upload_errs` - the number of records with mapping errors, or that were skipped for being unuploadable
* `batch_prefix` - The initial part of the object key representing the batch id. This is later used to check whether there are still any objects from this batch in the S3 bucket

Also prints this info to the screen at the end of the batch run.

==== Media Handling

You can transfer media and import files by including a URI in `mediaFileURI` column of your CSV. This works for:

* new media records created
* existing media records updated -- If existing media records have blobs attached they will be unattached and replaced by the new blob given.

WARNING: Ingesting records that trigger blob ingests remains flakier than ingesting other records. The speed at which records are ingested via this tool may be more likely to trigger throttling on the image downloading side. There are issues with ingesting blobs and finding/fixing failures even with CSV Importer, so there's still work to do...

==== Upload step followups
I have not run into enough problems on this step while developing/testing to see anything that can be systematized.

Clearly you'll need to check out any reported upload errors.

The number of reported ok uploads plus the number of reported upload errors should equal the number of successfully mapped records. There is https://github.com/lyrasis/collectionspace_migration_tools/issues/2[an issue] to add verification of this to the post-upload reporting.

=== Verify ingest completion and status

NOTE: There is no standalone/test command for this, since the entire functionality of this step depends on the context of a batch. 

In this step we do our best to determine:

- that all objects in a batch have been processed
- whether there were any failures and why
- whether any duplicates were added due to race conditions in the AWS Lambda processing

This step is a little tricky because the S3/Lambda side of things has no concept of a batch at all. We fake that by prefixing the object keys with the batch id.

Also, the AWS side of things generates logs but does not report back anything coherent about failures. At last check, the log message containing the actual reason for an ingest failure did not also contain the S3 object key or anything else we can easily use to connect an error message to information on our end.  

Expectations/assumptions:

- Successfully ingested records are removed from the S3 bucket
- Unsuccessfully ingested records remain in the S3 bucket

Based on this, we further assume:

. If the number of objects from the batch still in S3 bucket is changing, ingest is incomplete
. If the number of objects from the batch still in S3 bucket is zero, ingest is complete and all records were ingested successfully
. If the number of objects from the batch still in S3 bucket is no longer changing, but is non-zero, ingest is complete and the remaining records were not ingested successfully

The options for calling `thor batch:ingstat` are rather complex, and are fully described if you do `thor help batch:ingstat` or just read https://github.com/lyrasis/collectionspace_migration_tools/blob/main/lib/tasks/batch_ingstat.txt[the source for that help text]. Additionally, https://github.com/lyrasis/collectionspace_migration_tools/blob/main/spec/collectionspace_migration_tools/batch/ingest_status_checker_spec.rb[the automated tests for the ingest status checker] were written with the intent of reminding me what the heck it does when I have been away from this code for a while.

If you call `thor batch:ingstat` and the result is #1 from the above list, it tells you the ingest is not complete, and how many objects were in the bucket the last time it checked.

Otherwise, it determines the ingest is complete and writes the ingest related column values to `batches.csv` If the situation is #3 from the above list, it generates `ingest_report.csv`.

Then, unless this batch is for relation records, we run a database query to check for duplicate records of the batch's record type. If any duplicates are found, they are written to `duplicate_report.csv`.

IMPORTANT: Please refer to https://github.com/lyrasis/collectionspace_migration_tools/blob/main/doc/duplicates.adoc[the duplicates background doc] for the important assumptions and caveats here.

If you ran `thor batch:ingstat` with the `--dupedelete` flag, delete batches will be automatically created and run until there are no more duplicates.

==== Reports
===== `ingest_report.csv`
*Only generated if there are objects for the batch remaining in the S3 bucket and the number of those objects is no longer changing*

Includes the following columns:

* all columns from `upload_report.csv`
* `cmt_ingest_status`: failure (if object is still in S3 bucket), or success

===== `duplicate_report.csv`
*Only generated if duplicate records for the ingested record type are found in the database after ingest*

Contains one column. The header is the field name of the human-readable identifier field for the record type. The values in that column are any identifier values used in more than one record.

===== `batches.csv`
Populates the following columns:

* `ingest_done?` - timestamp of ingest check that determined ingest was finished
* `ingest_oks` - the number of records assumed to be successfully ingested
* `ingest_errs` - the number of records whose objects remain in the S3 bucket, indicating they were presumably not successfully ingested


Also prints this info to the screen at the end of the batch run.

==== Media Handling

You can transfer media and import files by including a URI in `mediaFileURI` column of your CSV. This works for:

* new media records created
* existing media records updated -- If existing media records have blobs attached they will be unattached and replaced by the new blob given.

WARNING: Ingesting records that trigger blob ingests remains flakier than ingesting other records. The speed at which records are ingested via this tool may be more likely to trigger throttling on the image downloading side. There are issues with ingesting blobs and finding/fixing failures even with CSV Importer, so there's still work to do...

==== Upload step followups
I have not run into enough problems on this step while developing/testing to see anything that can be systematized.

Clearly you'll need to check out any reported upload errors.

The number of reported ok uploads plus the number of reported upload errors should equal the number of successfully mapped records. There is https://github.com/lyrasis/collectionspace_migration_tools/issues/2[an issue] to add verification of this to the post-upload reporting.
