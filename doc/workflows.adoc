:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Workflows

toc::[]

.About CSV parsing errors
[NOTE]
====
`csvlint` is not Ruby 3-ready yet, so is not incorporated. I don't have time now to investigate integration of other potential CSV validation stuff. **If you get errors about your CSV not being readable or parseable, you'll need to look into it yourself.**
====

== Workflow overview
NOTE: strike-through formatting indicates the option/command is not yet implemented

* Produce CSV of record data using format of appropriate https://github.com/collectionspace/cspace-config-untangler/tree/main/data/templates[CSV template]
* Add batch to migration tool
** `thor batch:add --id obj12 --csv ~/path/to/source_data.csv --rectype collectionobject --action create`; OR
** +++<s>+++Manually add a new row to the batches CSV, populating `id`, `source_csv`, `mappable_rectype`, and `action` columns. Then run `thor batches:update_base_info` to have the `cacheable_type`, `cacheable_subtype`, and `rec_ct` columns populated by the tool.+++</s>+++
* Map batch to CS XML
** `thor batch:map --id obj12`
* Upload resulting CS XML to S3 bucket
** +++<s>+++`thor batch:upload --id obj12`+++</s>+++
* Verify ingest is complete (see step details on this for more info/options)
* Run ingest verification report
** +++<s>+++`thor batch:verify --id obj12`+++</s>+++
* If any records failed to ingest, run +++<s>+++`batch:cb`+++</s>+++ (cb = clear bucket)
* If there were any duplicates added, run +++<s>+++`batch:dd`+++</s>+++ (dd = delete duplicates. See important notes on this in step details below)

=== Managing batches

The batches CSV isn't really intended as a permanent record of all work done in a given instance. It can be useful to keep some completed batches with errors or warnings in the batches CSV as a reminder to check/handle the records that didn't work as expected the first time. But, once you are done with a batch and tasks related to it, it is best to move it out of the batches CSV.

There are two options for moving completed batches out of the batches CSV:

delete:: Deletes batch directory and deletes row for batch from batches CSV
+++<s>+++archive+++</s>+++:: Zips batch directory and moves it to `batch_data/archives` directory. Moves row for batch from batches CSV to `batch_data/archives/archived_batches.csv`

The `delete` and `archive` commands can both be run:

* for a single batch (`thor batch:delete --id obj12`); OR
* on all completed batches (`thor batches:archive`)

IMPORTANT: A batch's source CSV may live anywhere on the user's machine. It is not copied into the batch directory, so archiving a batch does archive the source CSV itself. However, the mapper, upload, and ingest reports generated by this tool append columns onto the end of the original data, so you can re-constitute the original source data from any of those.


== Step details
=== Map batch to CS XML
As part of batch workflow: +++<s>+++`thor batch:map --id obj12`+++</s>+++

For testing/standalone work: `thor map:csv --csv ~/path/to/source_data.csv --rectype collectionobject --action create`

==== Creates batch directory
+++<s>+++If part of batch workflow, batch directory name is the batch id.+++</s>+++

Otherwise batch directory is named with timestamp+rectype.

==== Preprocesses data
Verifies the following:

* The given CSV exists
* The first row can be parsed from the given CSV
* There are no headerless fields
* Header for required field is present

If any of the above are not true, the process will stop and you are informed of the problem.

Also:

* checks all fields/headers and
** prints count and list of known fields that can be mapped
** prints a warning listing any unknown fields that will be ignored (this may indicate a misnamed column)

==== Processes data
This step is batched (using SmarterCSV to read your CSV data in chunks) and multi-process. Each chunk is passed to a forked process, which sequentially processes each row in the chunk.

We use multiple processes rather than threads because this work is more CPU-bound than IO-bound.

Each row is passed to `collectionspace-mapper` which returns a `CollectionSpace::Mapper::Response` object that wraps the resulting XML (if it could be created), as well as any errors or warnings raised in the mapping process, and information on the record status in the given CS instance. If it is an existing record, the `Response` includes the record csid and uri for use in any subsequent API calls on the record.

==== Writes successfully mapped records to individual XML files

Successfully mapped records are written into the batch directory. The file name is the record identifier, Base64 encoded. This encoding is necessary because:

* some characters frequently used in record id values are not file name safe; and
* we leverage checking whether a file already exists before writing to avoid (and flag) records with duplicate identifiers in a batch.

If there are multiple records with the same ID in your batch, the first record will be fine. Subsequent records with the same ID will be treated as errors.

==== Reports
===== `mapper_report.csv`
Includes the following columns:

* all columns from source csv
* `cmt_rec_status`: record status from `CollectionSpace::Mapper::Response` (new or existing)
* `cmt_outcome`: `success` if XML was created and saved; `failure` if not
* `cmt_output_file`: name of XML file if created (so you can find a specific record easily)
* `cmt_s3_key`: string that will be used as the AWS S3 object key when XML file is uploaded to bucket. This is a Base64 url-safe encoded string created from:
** +++<s>+++batch id+++</s>+++
** services api path (includes record csid for updates/deletes, includes blobUri for media with files to ingest)
** identifier
** action (will control what API method is used to transfer record)
* `cmt_warnings`: issues to be aware of. They may be fine, or they may indicate something unexpected is going on. Note that you will see a warning here if:
** the batch has action = create, but the record status is existing
** the batch has action = update, but the record status is new
* `cmt_errors`: why a record mapping failed

IMPORTANT: You can continue to the next step if individual records fail. Those records will just be skipped in subsequent steps

===== `missing_terms.csv`

==== `batches.csv`
Populates the following columns:

* `mapped?` - timestamp entered
* `dir` - batch directory
* `map_errs` - the number of records with mapping errors
* `map_oks` - the number of records successfully mapped
* `map_warns` - the number of successfully mapped records with warnings

=== Upload
==== Media Handling

You can transfer media and import files by including a URI in `mediaFileURI` column of your CSV. This works for:

* new media records created
* existing media records updated -- If existing media records have blobs attached they will be unattached and replaced by the new blob given.
