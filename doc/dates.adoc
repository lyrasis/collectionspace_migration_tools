:toc:
:toc-placement!:
:toclevels: 4

ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

= Date handling workflows

toc::[]

== Requirements
Clone the https://github.com/kspurgin/emendate[Emendate] repository.

`cd` into the repo directory and run `bundle install`.

== Initial prep
Generate a CSV of all unique date values that need to be handled in your project.

This is a one-column CSV with no header. (If header is included, things will still work, but the result of processing the first "date" will always be an error because the header value will not be a valid, processable date.)

If any fields are multi-valued, make sure to split them into separate values for this purpose.

.Sample kiba-extend transforms to do this
[,ruby]
----
all_date_fields: %i[birthdate deathdate proddate otherdate]
 transform CombineValues::FromFieldsWithDelimiter,
   sources: all_date_fields,
   target: :alldates,
   sep: Kiba::Extend.delim,
   delete_sources: true
 transform Explode::RowsFromMultivalField,
   field: :alldates:,
   delim: Kiba::Extend.delim
transform Deduplicate::Table, field: :alldates
----

== Identify unsupported date patterns
Support for date patterns is being added as-needed to support given project datasets. Currently, it is likely some of your date patterns will not be supported.

The purpose of this step is to identify unsupported date patterns so that support can be added.

=== Run date values through basic batch processing script

* `cd` into Emendate repository directory
* `bundle exec ruby utils/batch_result_report.rb -i {path_to_dates_CSV}`

Output will be written to a CSV at the same path as your input, but with "_report" appended to the end of the file name.

[TIP]
====
If you know you need to apply certain processing options, you can pass these through when you run the script:

`bundle exec ruby utils/batch_result_report.rb -i {path_to_dates_CSV} -o "{edtf: true}"`
====

=== Identify date values lacking support

Isolate any rows that have:

* Value in `errs` column; OR
* `warnings` column value beginning with "Untokenizable sequences: "; OR
* `warnings` column value is "Unprocessable string"

From this set, remove any rows with "date values" that cannot reasonably be expected to be parsed as a date.

Examples of things to exclude:

* Eras, dynasties, named periods ("contemporary", "Ming dynasty", "Roaring Twenties")
** Exception: numbered century designations should be supported ("20th cent.")
* Roman numerals -- I have no intention to delve into dealing with these.
* "Date values" that include non-date info, such as date type, in the date string ("copyright 1993", "2021 reissue", "flourished 1738-1757")

Examples of things to include:

* Strings that specifically express an unknown/no date situation that may reasonably be found in other data sets. (strings like "n.d." and "unknown" are already handled)

[TIP]
====
Refer to Emendate's example set CSV, which reflects known/expected date patterns.

These patterns are NOT all currently supported. Many of them are **not** tagged as unsupported, however there are two special tagged categories:

If `tags_date_type` includes `currently_unparseable`, this indicates a known pattern that has been designated as not currently processable, and low priority for adding processing support.

If `tags_date_type` includes `unparseable`, this indicates there is no plan to support the pattern.
====

=== Add or request support for date patterns

You are welcome to submit Emendate PRs adding support for any date patterns.

However, realistically, at this point, you are probably going to send Kristina a list of unsupported date patterns.

**Please send a list of unique date _patterns_ for which support is needed!** That is, do not send "1972 12 15" and "1982 06 07" and "2022 03 18". Please pick one of each pattern.

**IDEALLY:**
If your date pattern already exists in Emendate's examples.csv, refer to the pattern used there.

If it isn't in the examples.csv, provide the following info that would be added to that file:

* an original example string
* expected full start and end date values when string is parsed
* expected certainty modifiers to be applied to parsed date
* if the string can be parsed differently given different https://github.com/kspurgin/emendate/blob/main/docs/options.adoc[Emendate options], specify the options and expected output for each variation.

== Identify date patterns not processed as expected

These will fall into two categories:

1. Patterns where the expected result is achieved by setting the relevant https://github.com/kspurgin/emendate/blob/main/docs/options.adoc[Emendate options]
2. Patterns that are just being handled wrong, or are being handled one possible way as if it is the only possible way (and thus need some handling option applied)

Refer to the https://github.com/kspurgin/emendate/blob/main/docs/use.adoc[Emendate Use] and https://github.com/kspurgin/emendate/blob/main/docs/options.adoc[Options] documentation to determine which category your patterns fall into.

Note any options needed for your data set. (When batch processing, the given options apply to the entire set)

Report any that need to be fixed.

== Wait for support for date patterns to be added
(Or jump in and make those PRs, lol)

== Translate your dates to Collectionspace date details

Produces a CSV that can be:

* passed to client for review/cleanup prior to merging into migration; OR
* merged directly into migration

How to:

* `cd` into Emendate repository directory
* `bundle exec ruby utils/translate_to_cspace_csv.rb -i {path_to_dates_CSV}`

Output will be written to a CSV at the same path as your input, but with "_translated" appended to the end of the file name.

[TIP]
====
If you know you need to apply certain processing options, you can pass these through when you run the script:

`bundle exec ruby utils/translate_to_cspace_csv.rb -i {path_to_dates_CSV} -o "{pluralized_date_interpretation: :broad}"`
====

== Merge into migration project data

This still assumes you may have numerous records in your migration with the same date value, and that we are here working with unique date strings not tied to specific records in the migration project---the only input column currently supported by the script is the date string.

The result of this script would be added as a supplied registry entry in your kiba-extend project, with `lookup_on: :orig`. Then you should use the following transform (or similar) to merge the fields in:

[source,ruby]
....
date_fields = %i[datedisplaydate dateperiod dateassociation datenote dateearliestsingleyear dateearliestsinglemonth dateearliestsingleday dateearliestsingleera dateearliestsinglecertainty dateearliestsinglequalifier dateearliestsinglequalifiervalue dateearliestsinglequalifierunit datelatestyear datelatestmonth datelatestday datelatestera datelatestcertainty datelatestqualifier datelatestqualifiervalue datelatestqualifierunit dateearliestscalarvalue datelatestscalarvalue scalarvaluescomputed]

fieldmap = date_fields.map{ |field| [field, field] }.to_h

transform Merge::MultiRowLookup,
  lookup: :translated_dates,
  keycolumn: :orig_date_field,
  fieldmap: fieldmap,
  multikey: true,
  null_placeholder: Kiba::Extend.nullvalue,
  delim: Kiba::Extend.delim
transform Delete::EmptyFieldValues,
  fields: date_fields,
  delim: Kiba::Extend.delim,
  usenull: true
transform Delete::EmptyFields
....

If you include Emendate in your kiba-extend migration project, it should be possible to merge translated date fields directly into the migration processing, but I haven't tried it yet.
====
