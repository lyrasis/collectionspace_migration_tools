# See the full documentation on these settings at
#   https://github.com/lyrasis/collectionspace_migration_tools/blob/main/doc/client_config_management.adoc

# ALL SITES: REQUIRED
# See /lib/collectionspace_migration_tools.rb for the current
#   KNOWN_CS_PROFILES list
profile: anthro

# ALL SITES: REQUIRED
# The main data directory for the migration project. **This directory must
#   already exist on your system.**
base_dir: fixturesdir/data

# LYRASIS-HOSTED CLIENT SITES: REQUIRED
# NON LYRASIS-HOSTED CLIENT SITES: OPTIONAL
site_name: examplesite

# ALL SITES: OPTIONAL
# Use an integer 1 - 15. Use unique numbers for active migration/data
#   projects. Run `thor config:redis_dbs` to see what db numbers are
#   in use by existing configs
redis_db_number: 3

# REQUIRED in order to do `thor batch upload` ingests
# The name of the "Fast Import" S3 bucket set up for use with this
# client.
fast_import_bucket: "bucket_name"

# OPTIONAL SETTING
# The name of the S3 bucket from which client media files will be
# ingested. This is optional, since some migrations do not include
# media files.
media_bucket: "other_bucket_name"

# OPTIONAL SETTING
# Path to the default directory for ingestable files for a project.
#   If set, you only need to specify the file name of files in this
#   directory when doing `thor batch:add`. You can create batches
#   based on files in other directories by giving the full file path
#   in the `thor batch:add` `--csv` option
ingest_dir: ingest

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Special connection settings
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# base_uri: https://core.dev.collectionspace.org/cspace-services
# username: admin@core.collectionspace.org
# password: Administrator
# db_host: db.domain.org
# db_username: admin
# db_password: yourdbpassword
# db_name: examplesite_examplesite

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Special settings for locating/identifying record mappers
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# cs_app_version: "8_1_1"
# profile_version: 9-0-0
# mapper_dir: ~/path/to/directory_containing_json_record_mappers

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# SPECIAL SETTINGS FOR CONTROLLING MAPPING, INGEST, ETC. BEHAVIOR
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# The following commented out settings show you what the default values
#   are for these settings. You can uncomment and change the value of a
#   setting to override its behavior.

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Mapping behavior
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Path to valid JSON batch config for mapping CSV to CSXML. This is
# optional and only needed if you need to tweak the mapper options.
# For documentation on the available options, see:
# https://github.com/collectionspace/collectionspace-mapper/blob/migration-tooling/doc/batch_configuration.adoc
#
# When using this application to create CSXML, the following batch
# config options are always sent to the mapper:
#
#   - `status_check_method` = `cache`
#   - `search_if_not_cached` = `false`
#
# If you are NOT providing a custom config, comment out the following line.
# batch_config_path: fixturesdir/client_batch_config.json

# If true, when you run `thor batch:map`, the first thing that will
#   happen is fresh data for record types required by the type and
#   data you are mapping will be pulled from the database and
#   cached.
# If commented out or set to false, caches will NOT be cleared and
#   refreshed prior to data mapping when called when `thor
#   batch:map` is called. This may be preferable if the scale of
#   the instance is very large, so it takes a long time to refresh
#   cache data
# Note that you can control whether this happens per-batch via the
#   `--autocache` option on the `thor map:batch` command. The
#   default value of the per-batch option is whatever you have set here.
# auto_refresh_cache_before_mapping: true

# If true, caches will be cleared before they are auto refreshed.
# This is the safest option, as it will ensure deleted items are
# not still in the cache, and the correct values are cached for any
# duplicates since cleaned up. If you know only additions/updates
# have been made, you can set to false to save a bit of time. This
# has no effect if `auto_refresh_cache_before_mapping = false`.
# Note that you can control whether this happens per-batch via the
# `--clearcache` option on the `thor map:batch` command. The
# default value of the per-batch option is whatever you have set
# here.
# clear_cache_before_refresh: true

# Column/field delimiter for input "CSV" files
# csv_delimiter: ","

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Ingest behavior
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Controls pause/sleep time after uploading to S3 each media
# handling procedure record that has an associated mediaFileURI. A
# pause is required with batches over a certain size to minimize
# file storage and derivative generation failures. THE VALUE IS IN
# MS.
#
# The value here is based on CS Services code that waits 500 ms for
# derivatives to be generated:
# https://cs.github.com/collectionspace/application/blob/5fb3d0ffc750bfdf19e57057ac3dc65827c01df0/cspi-services/src/main/java/org/collectionspace/chain/csp/persistence/services/BlobStorage.java#L95
#
# If you are still seeing blob file storage and derivative
# generation errors, increase this delay. Uploads to S3 are
# threaded, so {system_config/max_threads} (default = 5 for media
# with blobs) upload queues are happening simultaneously. This may
# increase the time needed for derivative generation.
#
# The lambda process that grabs records out of the S3 bucket and
# makes the API calls to ingest has some limit of how many records
# are processed at once, but this defaults to making calls far
# faster than CS/Nuxeo can produce and store derivatives.
#
# If your mediaFileURI values point to downloadable files (the
# usual case), you may be able to decrease this number, as there is
# delay during the download of each file for ingest. If the
# mediaFileURIs are to local file system locations, this might need
# to be higher to build in the padding you *aren't* getting since
# the files don't have to be downloaded.
# media_with_blob_upload_delay: 500

# Maximum number of threads to use when uploading media-with-blob
# objects to S3 for ingest. It's more straightforward to control
# the speed of media uploads/ingests here than asking for the S3
# bucket and/or AWS Lambda parameters to be changed. The value set
# here will interact with the `media_with_blob_upload_delay` value
# set above, and it may be worth tweaking them to find a good
# balance that works.
# max_media_upload_threads: 5

# Delimiter used to encode S3 object keys. Must be the same as the
# delimiter specified for the bucket in the S3/AWS Lambda config,
# as it is needed to decode the object name and send the API call
# to do the object ingest. Currently is always set to "|"
#  s3_delimiter: "|"

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# API Client Behavior
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# The following settings are defined in the
#   `CLIENT_CONFIG_DEFAULTS` section of
#   lib/collectionspace_migration_tools/configuration.rb. They are
#   included here for documentation purposes, and to make it easy
#   to override the default values as needed. The values shown
#   below are the default values. If you want to override them,
#   uncomment the setting and change the value.

# Controls page size of result sets returned by collectionspace-client
# page_size: 50

# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Stoage of batch-related information
# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# Directory in which to store generated xml and reports for each
#   batch. A directory will be created within this directory for
#   each batch you process
# If only a directory name is given, this directory will be created
#   as a sub-directory inside your `base_dir`. If you give a full
#   path to a directory, that directory is used (and it must
#   already exist)
# batch_dir: batch_data

# Whether to write data on batches you delete to an archive CSV. This allows
# for passive gathering of rough ingest time stats.
# archive_batches: true

# File name for the batch archive file. This file will be created in the
# defined `base_dir`. If `archive_batches` is false, this has no effect.
# batch_archive_filename: batches_archive.csv
