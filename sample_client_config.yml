client:
  base_uri: https://core.dev.collectionspace.org/cspace-services
  username: admin@core.collectionspace.org
  password: Administrator
  page_size: 50
  
  # The redis_db_number should be unique per config. This allows you to quickly clear the caches
  #  for one instance, without affecting the caches for other instances.
  # It's on you to ensure these numbers are unique across your configs. The app doesn't check this
  redis_db_number: 3

  # The next three settings are used to grab required record mapping instructions from
  #   https://github.com/collectionspace/cspace-config-untangler/tree/main/data/mappers/community_profiles/
  # Refer to how the mappers (i.e. the mapping instructions for each record type) are organized there
  #   to find the proper values to put in here
  
  # The release number for the instance, not the UI version or profile version
  # Replace the period with an underscore
  # This must be in quotes or 7_0 will get interpreted as 70 for some reason.
  cs_version: "7_0"

  # Should match the directory names that organize mappers per release
  profile: anthro

  # Should match the form used in file names of individual .json files
  profile_version: 5-0-0

  # The settings in the next section control where files for this project will be saved

  # The main directory for the migration project. All other directories can be specified
  #   relative to this
  # This directory must already exist on your system.
  base_dir: ~/code/cs/migration_tools/data

  # Batches are configured in a CSV file. As you move through the steps of a batch (mapping, upload,
  #   checking ingest status, verifying lack of duplicates), information will get added to the CSV
  #   by the application to assist you in keeping track of things. This is the path for this CSV
  # If you do not specify another location for this file, `batches.csv` will be created in your
  #   base_dir.
  # NOTE: It is very important that this path be different for each client/instance you are working on
  #batch_csv: ~/code/cs/migration_tools/data/batches.csv
  
  # You can enter a subdirectory relative to base_dir OR a full path to an existing directory anywhere on
  #   your system. If the value as given expands to an existing directory on your machine, that's what
  #   will be used. If not, it'll used subdir value appended to base_dir. The subdirectory will be created
  #   if it does not exist.
  mapper_dir: record_mappers

  # directory in which to store generated xml and reports for each batch
  # a sub-directory will be created within this directory for each batch
  batch_dir: batch_data

  # path to valid JSON batch config for mapping CSV to CSXML
  # See https://github.com/collectionspace/collectionspace-mapper/blob/migration-tooling/doc/batch_configuration.adoc for options
  # This is optional and only needed if you need to tweak the mapper options.
  # `status_check_method` is automatically set to `cache` when using this application to create CSXML.
  # If you are NOT providing a custom config, comment out the following line.
  # batch_config_path: ~/code/cs/migration_tools/spec/support/fixtures/client_batch_config.json

  # If true, when you run `thor batch:map`, the first thing that will happen is your caches will be cleared. Then,
  #   fresh data for record types required by the type and data you are mapping will be pulled from the database
  #   and cached.
  # If commented out or set to false, caches will NOT be cleared and refreshed prior to data mapping when called when
  #   `thor batch:map` is called. This may be preferable if the scale of the instance is very large, so it takes a long
  #   time to refresh cache data
  # Note that you can control whether this happens per-batch via the `--autocache` option on the `thor map:batch` command.
  #   The default value of this option is whatever you have set here.
  auto_refresh_cache_before_mapping: true

  # column/field delimiter for input "CSV" files
  csv_delimiter: ","

  # AWS S3 Bucket settings - required for using this tool to transfer records to the client instance
  # One S3 Bucket per instance must be set up -- Credentials are in LastPass entry `S3 IMPORT BUCKETS`
  s3_bucket: "bucket_name"
  s3_region: "us-west-2"
  s3_key: "value_of_bucket_key"
  s3_secret: "value_of_secret_for_bucket_key"
  s3_delimiter: "|"
database:
  port: 5432
  db_password: dbpassword
  db_name: cs_cs
  db_host: cs_staging_db.cs-instance.org
  db_user: dbadminuser
  db_connect_host: localhost
  bastion_user: user_for_ssh_tunneling_through_bastion
  bastion_host: cs_staging_db-bastion.cs-instance.org
